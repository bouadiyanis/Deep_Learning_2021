{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You must implement a test executable named test.py that imports your framework and"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Generates a training and a test set of 1,000 points sampled uniformly in [0,1]^2, each with a\n",
    "label 0 if outside the disk centered at (0.5, 0.5) of radius 1/rtsquare(2π), and 1 insid\n",
    "\n",
    "• builds a network with two input units, two output units, three hidden layers of 25 units \n",
    "\n",
    "• trains it with MSE, logging the loss\n",
    "\n",
    "• computes and prints the final train and the test errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are free to come with any new ideas you want, and grading will reward originality. The suggested simple structure is to define a class\n",
    "\n",
    "#### class Module(object):\n",
    "    def forward(self, *input): \n",
    "        raise NotImplementedError\n",
    "    def backward(self, *gradwrtoutput): \n",
    "        raise NotImplementedError\n",
    "    def param(self): \n",
    "        return []\n",
    "\n",
    "and to implement several modules and losses that inherit from it.\n",
    "\n",
    "Each such module may have tensor parameters, in which case it should also have for each a similarly sized tensor gradient to accumulate the gradient during the back-pass, and\n",
    "\n",
    "• forward should get for input, and returns, a tensor or a tuple of tensors.\n",
    "\n",
    "• backward should get as input a tensor or a tuple of tensors containing the gradient of the loss with respect to the module’s output, accumulate the gradient w.r.t. the parameters, and return a tensor or a tuple of tensors containing the gradient of the loss w.r.t. the module’s input.\n",
    "\n",
    "• param should return a list of pairs, each composed of a parameter tensor, and a gradient tensor of same size. \n",
    "\n",
    "This list should be empty for parameterless modules (e.g. ReLU).\n",
    "\n",
    "Some modules may requires additional methods, and some modules may keep track of information from the forward pass to be used in the backward.\n",
    "\n",
    "You should implement at least the modules Linear (fully connected layer), ReLU, Tanh, Sequential to combine several modules in basic sequential structure, and LossMSE to compute the MSE loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "torch.set_grad_enabled(False)\n",
    "from torch import empty\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "519 481\n"
     ]
    }
   ],
   "source": [
    "# Create a training set: 1000 points in [0,1]*[0,1]\n",
    "x_train = torch.rand(1000)\n",
    "y_train = torch.rand(1000)\n",
    "\n",
    "# Create a testing set: 1000 points in [0,1]*[0,1]\n",
    "x_test = torch.rand(1000)\n",
    "y_test = torch.rand(1000)\n",
    "\n",
    "# Labelization of the training and testing set: Label '1' if the point is inside the circle, '0' if outside\n",
    "cercle_center=torch.tensor([[0.5],[0.5]])\n",
    "label_0=[]\n",
    "label_1=[]\n",
    "a = torch.square(x_train-cercle_center[0])\n",
    "b = torch.square(y_train-cercle_center[1])\n",
    "R = 1/math.sqrt(2*math.pi)\n",
    "\n",
    "\n",
    "for i in range(len(x_train)):\n",
    "    if a[i] + b[i] <= R*R:\n",
    "        label_1.append([x_train[i],y_train[i]])\n",
    "    else:\n",
    "        label_0.append([x_train[i],y_train[i]])\n",
    "\n",
    "print(len(label_0),len(label_1))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE loss function\n",
    "#y_prediction= tanh()\n",
    "#y_target= \n",
    "\n",
    "def MSE_loss(y_pred,y_true):\n",
    "    loss= (y_pred-y_true).pow(2).mean() \n",
    "    return loss\n",
    "\n",
    "#loss= MSE_loss(y_prediction, y_target)\n",
    "\n",
    "#print (\"MSE error is: \", loss )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Connection:\n",
    "    def __init__(self, connectedNeuron):\n",
    "        self.connectedNeuron = connectedNeuron\n",
    "        ##self.weight = np.random.normal()\n",
    "        self.weight = torch.rand(1)\n",
    "        self.dWeight = torch.tensor([0])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    eta = torch.tensor([0.001])\n",
    "    alpha = torch.tensor([0.01])\n",
    "\n",
    "    def __init__(self, layer):\n",
    "        self.dendrons = []\n",
    "        self.error = torch.tensor([0])\n",
    "        self.gradient = torch.tensor([0])\n",
    "        self.output = torch.tensor([0])\n",
    "        if layer is None:\n",
    "            pass\n",
    "        else:\n",
    "            for neuron in layer:\n",
    "                con = Connection(neuron)\n",
    "                self.dendrons.append(con)\n",
    "\n",
    "    def addError(self, err):\n",
    "        self.error = self.error + err\n",
    "        \n",
    "#----------------------------------------------\n",
    "# Not necessary     \n",
    "###    def sigmoid(self, x):\n",
    "###        return 1 / (1 + math.exp(-x * 1.0))\n",
    "\n",
    "###    def dSigmoid(self, x):\n",
    "###        return (math.exp(-x * 1.0)) / (1 + math.exp(-x * 1.0))**2\n",
    "        ##return x*(1-x)\n",
    "#----------------------------------------------\n",
    "    \n",
    "    def tanh(self, x):\n",
    "        return math.tanh(x)\n",
    "\n",
    "    def dtanh(self, x):\n",
    "        return 1 - math.tanh(x)**2\n",
    "\n",
    "#----------------------------------------------\n",
    "    \n",
    "    def Relu(self, x):\n",
    "        return max(0,x)\n",
    "\n",
    "    def dRelu(self, x):\n",
    "        if x<0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "#----------------------------------------------\n",
    "\n",
    "    def setError(self, err):\n",
    "        self.error = err\n",
    "\n",
    "    def setOutput(self, output):\n",
    "        self.output = output\n",
    "\n",
    "    def getOutput(self):\n",
    "        return self.output\n",
    "                \n",
    "    def backPropagate(self):\n",
    "        ##self.gradient = self.error * self.dSigmoid(self.output);\n",
    "        self.gradient = self.error * self.dtanh(self.output);\n",
    "        for dendron in self.dendrons:\n",
    "            dendron.dWeight = Neuron.eta * (dendron.connectedNeuron.output * self.gradient) + self.alpha * dendron.dWeight;\n",
    "            dendron.weight = dendron.weight + dendron.dWeight;\n",
    "            dendron.connectedNeuron.addError(dendron.weight * self.gradient);\n",
    "        self.error = 0;\n",
    "        \n",
    "    def feedForward(self):\n",
    "        sumOutput = torch.tensor([0])\n",
    "        if len(self.dendrons) == 0:    \n",
    "            return\n",
    "        for dendron in self.dendrons:\n",
    "            #print(len(self.dendrons))\n",
    "            sumOutput = sumOutput +  dendron.connectedNeuron.getOutput() * dendron.weight;\n",
    "        ##self.output = self.sigmoid(sumOutput)\n",
    "        self.output = self.tanh(sumOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, topology):\n",
    "        self.layers = []\n",
    "        for numNeuron in topology:\n",
    "            layer = []\n",
    "            for i in range(numNeuron):\n",
    "                if (len(self.layers) == 0):\n",
    "                    layer.append(Neuron(None))\n",
    "                else:\n",
    "                    layer.append(Neuron(self.layers[-1]))\n",
    "            layer.append(Neuron(None))\n",
    "            layer[-1].setOutput(1)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "    def setInput(self, inputs):\n",
    "        for i in range(len(inputs)):\n",
    "            self.layers[0][i].setOutput(inputs[i])\n",
    "\n",
    "    def feedForward(self):\n",
    "        for layer in self.layers[1:]:\n",
    "            for neuron in layer:\n",
    "                neuron.feedForward();\n",
    "\n",
    "    def backPropagate(self, target):\n",
    "        for i in range(len(target)):\n",
    "            self.layers[-1][i].setError(target[i] - self.layers[-1][i].getOutput())\n",
    "        for layer in self.layers[::-1]:\n",
    "            for neuron in layer:\n",
    "                neuron.backPropagate()\n",
    "\n",
    "    def getError(self, target):\n",
    "        err = 0\n",
    "        for i in range(len(target)):\n",
    "            e = (target[i] - self.layers[-1][i].getOutput())\n",
    "            err = err + e ** 2\n",
    "        err = err / len(target)\n",
    "        err = math.sqrt(err)\n",
    "        return err\n",
    "\n",
    "    def getResults(self):\n",
    "        output = []\n",
    "        for neuron in self.layers[-1]:\n",
    "            output.append(neuron.getOutput())\n",
    "        output.pop()\n",
    "        return output\n",
    "\n",
    "    def getThResults(self):\n",
    "        output = []\n",
    "        for neuron in self.layers[-1]:\n",
    "            o = neuron.getOutput()\n",
    "            if (o >= 0.5):\n",
    "                o = 1\n",
    "            else:\n",
    "                o = 0\n",
    "            output.append(o)\n",
    "        output.pop()\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "error:  0.9999999999998671\n",
      "type 1st input :[0, 0]\n",
      "type 2nd input :[0, 1]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only integer tensors of a single element can be converted to an index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-5122363aa9bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-5122363aa9bb>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"type 2nd input :\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeedForward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'net.getThResults()'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetThResults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-22a175320ce6>\u001b[0m in \u001b[0;36mfeedForward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mneuron\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                 \u001b[0mneuron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeedForward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackPropagate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-6ee6bf8c90a7>\u001b[0m in \u001b[0;36mfeedForward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdendron\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdendrons\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;31m#print(len(self.dendrons))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0msumOutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msumOutput\u001b[0m \u001b[0;34m+\u001b[0m  \u001b[0mdendron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnectedNeuron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdendron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0;31m##self.output = self.sigmoid(sumOutput)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msumOutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer tensors of a single element can be converted to an index"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \n",
    "    topology = []\n",
    "    \n",
    "    topology.append(2)     # Input layer\n",
    "    topology.append(25)    # Hidden layer 1\n",
    "    topology.append(25)    # Hidden layer 2\n",
    "    topology.append(25)    # Hidden layer 3\n",
    "    topology.append(1)     # Output layer\n",
    "\n",
    "    net = Network(topology)\n",
    "    \n",
    "    Neuron.eta = torch.tensor([0.09])             # Learning rate of the neurons\n",
    "    Neuron.alpha = torch.tensor([0.015])          # momentum factor of the neurons\n",
    "    \n",
    "    \n",
    "    while True:           \n",
    "\n",
    "        err = 0\n",
    "        ##inputs = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
    "        ##outputs = [[0], [0], [1], [0]]\n",
    "        inputs = [[0, 0]]\n",
    "        outputs = [[0]]\n",
    "        print(len(inputs))\n",
    "        print(len(outputs))\n",
    "        \n",
    "        for i in range(len(inputs)):\n",
    "            net.setInput(inputs[i])\n",
    "            net.feedForward()\n",
    "            net.backPropagate(outputs[i])\n",
    "            err = err + net.getError(outputs[i])\n",
    "            \n",
    "        print (\"error: \", err)\n",
    "        \n",
    "        if err <= 3:    ########### INITIALEMENT A 0.01 mais mis à 3 pour accelerer les tests\n",
    "            break\n",
    "\n",
    "            \n",
    "    while True:\n",
    "        \n",
    "        a = input(\"type 1st input :\")\n",
    "        b = input(\"type 2nd input :\")\n",
    "        net.setInput([a, b])\n",
    "        net.feedForward()\n",
    "        print ('net.getThResults()', net.getThResults())\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
